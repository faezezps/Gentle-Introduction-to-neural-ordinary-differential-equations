معادله دیفرانسیل معمولی (ODE) یک نوآوری مهم در زمینه یادگیری ماشین است. این مفهوم که در سال ۲۰۱۸ معرفی شد، دیدگاه جدیدی به مدل‌های شبکه عصبی ارائه می‌دهد و آنها را به سیستم‌های دینامیکی پیوسته مرتبط می‌سازد.

# پیش‌زمینه
## شبکه عصبی رزنت (ResNet)
![Basic Resnet](imgs/basicresnet.JPG)

فرمول: 

$x_{t+1} = x_t + f(x_t)$

هنگام یادگیری انتقال از یک ورودی به خروجی مستقیم بعدی، تمرکز بر تفاوت بین این دو ساده تر است. این رویکرد تضمین می‌کند که گرادیان $\frac{dx_{t+1}}{dx_t} = 1 + \frac{df}{dx_t}$  به طور مؤثر از مشکلات مربوط به ناپدید شدن یا انفجار گرادیان‌ها جلوگیری می‌کند.

توضیح اضافی:
شبکه‌های عصبی پیچیده با لایه‌های عمیق گاهی با مشکلاتی مانند ناپدید شدن یا انفجار گرادیان مواجه می‌شوند. این مشکل باعث می‌شود یادگیری شبکه با مشکلاتی روبرو شود. ResNet با افزودن اتصالات کوتاه (skip connections) بین لایه‌ها این مشکل را حل می‌کند. این اتصالات به مدل اجازه می‌دهند تا به جای یادگیری مستقیم ویژگی‌ها، تفاوت بین ورودی و خروجی را بیاموزد که باعث پایداری بیشتر شبکه و بهبود یادگیری می‌شود.

## از حوزه گسسته به حوزه پیوسته

 در معادلات فوق $t$ نمایانگر شاخص لایه است. به عنوان مثال، $x_{t+1}$ ممکن است اولین لایه ResNet و  $x_{t+2}$ ممکن است دومین لایه ResNet در یک شبکه بزرگتر باشد.

بیایید شاخص لایه را به عنوان زمان پیوسته در نظر بگیریم. فعلاً ریاضیات انتزاعی را بپذیرید. درک بصری بعداً ارائه خواهد شد.

$x_{t+1} - x_t = f(x_t)$

$\frac{x_{t+1} - x_t}{t+1 - t} = f(x_t)$ 

***مخرج برابر با t+1-t = 1 است، بنابراین هیچ تغییری در معادله رخ نمی‌دهد.***

توضیح اضافی:
در شبکه‌های عصبی معمولاً لایه‌ها به صورت گسسته دیده می‌شوند، یعنی هر لایه به طور جداگانه در نظر گرفته می‌شود. اما در مدل‌های پیشرفته‌تر، می‌توان لایه‌ها را به صورت پیوسته در نظر گرفت. این به معنای آن است که به جای اینکه هر لایه را به عنوان یک نقطه مجزا ببینیم، می‌توانیم یک تغییر پیوسته در زمان بین لایه‌ها تصور کنیم. این مفهوم به ویژه در مدل‌های دیفرانسیلی عصبی مفید است، جایی که تغییرات پیوسته در طول زمان می‌توانند نمایانگر جریان اطلاعات در شبکه باشند.

با تصور اینکه $t$ نمایانگر گام‌های زمانی گسسته به جای شاخص‌های لایه باشد، می‌توانیم "گام زمانی" بین دو گام متوالی ($t$ و $t+1$) را کاهش دهیم و به نتیجه زیر برسیم:

$\frac{dx(t)}{dt} = f(x)$

این یک معادله دیفرانسیل معمولی ساده است. ما اکنون از یک شبکه لایه پنهان گسسته به یک توصیف از یک سیستم دینامیکی رفتیم.

چگونه می‌توانیم این سیستم را تفسیر کنیم؟ ورودی، میانی، و خروجی نهایی چیست؟


# معادله دیفرانسیل عادی عصبی (NODE)

تابع عمومی  $f$ در معادله فوق اکنون می‌تواند توسط یک شبکه عصبی پارامتری نمایش داده شود. به عنوان مثال، می‌توان از یک لایه کاملاً متصل ساده یا یک لایه کانولوشن دو بعدی با یک تابع فعال‌سازی غیرخطی دو بعدی استفاده کرد.
توجه: در نهایت، ما می‌خواهیم وزن‌ها و بایاس‌های داخلی را مشابه یک شبکه عصبی متعارف آموزش دهیم.
معادله NODE را می‌توان به صورت زیر بیان کرد:

$\frac{dx(t)}{dt} = f(x(t),t;\theta)$

در اینجا،  $\theta$ نمایانگر پارامترهای داخلی یعنی وزن‌ها و بایاس‌ها است.

توضیح اضافی:
در معادلات دیفرانسیل عادی عصبی (NODE)، یک شبکه عصبی به عنوان تابع \( f \) عمل می‌کند که می‌تواند تغییرات در حالت سیستم دینامیکی را در طول زمان مدل کند. این شبکه با استفاده از وزن‌ها و بایاس‌های پارامتری، تغییرات پیوسته را یاد می‌گیرد و به ما امکان می‌دهد که سیستم‌های پیچیده‌تری را مدل کنیم. به عبارت دیگر، ما به جای استفاده از لایه‌های گسسته، از شبکه‌های عصبی استفاده می‌کنیم تا معادلات دیفرانسیل را حل کنیم و یک نمای پیوسته از تغییرات در زمان به دست آوریم.

## گذر به جلو (Forward-Pass)
در شبکه‌های عصبی مصنوعی متعارف، خروجی لایه فعلی ورودی لایه بعدی است. در اینجا، ما فقط یک "لایه پنهان" داریم. 



به جای تبدیل خروجی (ورودی) میانی  $x_t$ از طریق لایه‌های مختلف، که توسط معماری تعریف شده‌اند، ما $x$  را توسط همان لایه تبدیل می‌کنیم **اما** در طول گام‌های زمانی متوالی.

توضیح اضافی:
در این مدل، ورودی $X_0$ در زمان $t=0$ داده می‌شود و سپس در طول گام‌های زمانی مختلف با استفاده از همان لایه پردازش می‌شود. این روش به جای استفاده از لایه‌های مختلف، از یک لایه واحد برای تغییر حالت در طول زمان استفاده می‌کند، که منجر به یک نمایش پیوسته و دینامیک از داده‌ها می‌شود.

برای درک بهتر این موضوع، نگاهی به یک گذر جلو (Forward-Pass) خواهیم داشت. 
ورودی (به عنوان مثال، تصویر RGB) را به صورت $x(t=0):= x_0$ تعریف کنید.
مسیر واقعی، گرادیان محلی، و مسیر پیش‌بینی شده در شکل زیر به ترتیب با رنک سیاه، سبز و زرد نشان داده شده‌اند.

![Forward pass NODE](imgs/forward.png)


اکنون، تابع f می‌تواند یک لایه عصبی یا ترکیبی از لایه‌های پارامتری قابل آموزش باشد، به شرطی که شکل ورودی با شکل خروجی مطابقت داشته باشد. زیرا خروجی یک گام زمانی ورودی گام زمانی بعدی خواهد بود.

ی
ک گذر جلو (Forward-Pass)، خروجی (و مسیر) را با استفاده از روش معروف [اویلر](https://en.wikipedia.org/wiki/Euler_method) محاسبه می‌کند.
گرادیان‌ها (پیکان‌های سبز) می‌توانند محاسبه شوند چون فرمول  $\frac{dx(t)}{t}$  را در اختیار داریم. 

ابتدا گرادیان نقطه شروع اولیه  $x_0$  را محاسبه می‌کنیم.

نتیجه  $f(x_0)$ نقطه اولیه $x_0$  را به نقطه بعدی مسیر هدایت می‌کند. با انجام این کار برای چندین گام زمانی و با حرکت در جهت گرادیان در اندازه گام از پیش تعیین شده ی $h$، می‌توانیم کل مسیر دینامیکی سیستم را به دست آوریم.

محاسبه نقطه دوم مسیر:

$x_1 = f(x_0) \cdot h + x_0$

در نهایت به آخرین نقطه $x(t=T)=x_T$ می‌رسیم.

توجه: از آنجا که در گام‌های گسسته تعریف شده توسط $h$ حرکت می‌کنیم، یک خطای عددی خواهیم داشت. یعنی برای $h\rightarrow0$ روش مسیر کامل را می‌دهد.  می‌توانیم $h$ را بر اساس نیاز خود تعریف کنیم.

توضیح اضافی:
روش اویلر یک روش ساده و موثر برای تقریب مسیرهای دینامیک است. با استفاده از گرادیان محاسبه شده در هر نقطه و حرکت در جهت آن گرادیان به اندازه گام  $h$، می‌توانیم مسیر حرکت سیستم را شبیه‌سازی کنیم. این فرآیند به تدریج و با تکرار در طول زمان انجام می‌شود تا به نقطه نهایی برسیم.

## تفسیر

به جای یادگیری تبدیل داده‌های ورودی از یک لایه به لایه دیگر، ما سعی می‌کنیم دینامیک‌های اصلی تبدیل را یاد بگیریم. وزن‌ها و بایاس‌های یک لایه NODE شامل اطلاعاتی هستند که نه تنها تبدیل داده‌ها از گام فعلی به گام بعدی را یاد می‌گیرند، بلکه تبدیل داده‌ها از هر گام به گام بعدی را نیز شامل می‌شوند.

![ResNet vs NODE](imgs/ResNetvsNODE.JPG)

به جای افزودن لایه‌های بیشتر برای قوی‌تر کردن شبکه، NODE فقط یک "لایه" (یا زیر شبکه) دارد و خروجی را "در طول زمان" محاسبه می‌کند. دینامیک‌های تبدیل مستقیماً یاد گرفته می‌شوند و نیازی به تقریب زدن توسط یک معماری بزرگ و گام‌های گسسته نیست.

تصویر بالا از مقاله اصلی تفاوت در تبدیل داده‌ها را نشان می‌دهد. ResNet‌ها (سمت چپ) سعی می‌کنند تبدیل‌ها را در هر عمق (محور y) بیاموزند. NODE (سمت راست) سعی می‌کند میدان برداری زیرین را بیاموزد. بنابراین، داده‌های میانی می‌توانند به‌طور غیر یکنواخت در طول محور عمق (که زمان پیوسته را نشان می‌دهد) پخش شوند.

توضیح اضافی:
تفاوت اصلی بین ResNet و NODE این است که در ResNet، تبدیل‌ها در هر عمق یاد گرفته می‌شوند، در حالی که در NODE، میدان برداری زیرین یاد گرفته می‌شود. بنابراین، داده‌های میانی می‌توانند به صورت غیر یکنواخت در طول محور عمق (که زمان پیوسته را نشان می‌دهد) پخش شوند. این تفاوت‌ها باعث می‌شوند که NODE‌ها بتوانند مدل‌های پیچیده‌تر و پایدارتری را ایجاد کنند.
