معادله دیفرانسیل معمولی (ODE) یک نوآوری مهم در زمینه یادگیری ماشین است. این مفهوم که در سال ۲۰۱۸ معرفی شد، دیدگاه جدیدی به مدل‌های شبکه عصبی ارائه می‌دهد و آنها را به سیستم‌های دینامیکی پیوسته مرتبط می‌سازد.

# پیش‌زمینه
## شبکه عصبی رزنت (ResNet)
![Basic Resnet](imgs/basicresnet.JPG)

فرمول: 

$x_{t+1} = x_t + f(x_t)$

هنگام یادگیری انتقال از یک ورودی به خروجی مستقیم بعدی، تمرکز بر تفاوت بین این دو ساده تر است. این رویکرد تضمین می‌کند که گرادیان $\frac{dx_{t+1}}{dx_t} = 1 + \frac{df}{dx_t}$  به طور مؤثر از مشکلات مربوط به ناپدید شدن یا انفجار گرادیان‌ها جلوگیری می‌کند.

توضیح اضافی:
شبکه‌های عصبی پیچیده با لایه‌های عمیق گاهی با مشکلاتی مانند ناپدید شدن یا انفجار گرادیان مواجه می‌شوند. این مشکل باعث می‌شود یادگیری شبکه با مشکلاتی روبرو شود. ResNet با افزودن اتصالات کوتاه (skip connections) بین لایه‌ها این مشکل را حل می‌کند. این اتصالات به مدل اجازه می‌دهند تا به جای یادگیری مستقیم ویژگی‌ها، تفاوت بین ورودی و خروجی را بیاموزد که باعث پایداری بیشتر شبکه و بهبود یادگیری می‌شود.

## از حوزه گسسته به حوزه پیوسته

 در معادلات فوق $t$ نمایانگر شاخص لایه است. به عنوان مثال، $x_{t+1}$ ممکن است اولین لایه ResNet و  $x_{t+2}$ ممکن است دومین لایه ResNet در یک شبکه بزرگتر باشد.

بیایید شاخص لایه را به عنوان زمان پیوسته در نظر بگیریم. فعلاً ریاضیات انتزاعی را بپذیرید. درک بصری بعداً ارائه خواهد شد.

$x_{t+1} - x_t = f(x_t)$

$\frac{x_{t+1} - x_t}{t+1 - t} = f(x_t)$ 

***مخرج برابر با t+1-t = 1 است، بنابراین هیچ تغییری در معادله رخ نمی‌دهد.***

توضیح اضافی:
در شبکه‌های عصبی معمولاً لایه‌ها به صورت گسسته دیده می‌شوند، یعنی هر لایه به طور جداگانه در نظر گرفته می‌شود. اما در مدل‌های پیشرفته‌تر، می‌توان لایه‌ها را به صورت پیوسته در نظر گرفت. این به معنای آن است که به جای اینکه هر لایه را به عنوان یک نقطه مجزا ببینیم، می‌توانیم یک تغییر پیوسته در زمان بین لایه‌ها تصور کنیم. این مفهوم به ویژه در مدل‌های دیفرانسیلی عصبی مفید است، جایی که تغییرات پیوسته در طول زمان می‌توانند نمایانگر جریان اطلاعات در شبکه باشند.

با تصور اینکه $t$ نمایانگر گام‌های زمانی گسسته به جای شاخص‌های لایه باشد، می‌توانیم "گام زمانی" بین دو گام متوالی ($t$ و $t+1$) را کاهش دهیم و به نتیجه زیر برسیم:

$\frac{dx(t)}{dt} = f(x)$

این یک معادله دیفرانسیل معمولی ساده است. ما اکنون از یک شبکه لایه پنهان گسسته به یک توصیف از یک سیستم دینامیکی رفتیم.

چگونه می‌توانیم این سیستم را تفسیر کنیم؟ ورودی، میانی، و خروجی نهایی چیست؟


# معادله دیفرانسیل عادی عصبی (NODE)

تابع عمومی  $f$ در معادله فوق اکنون می‌تواند توسط یک شبکه عصبی پارامتری نمایش داده شود. به عنوان مثال، می‌توان از یک لایه کاملاً متصل ساده یا یک لایه کانولوشن دو بعدی با یک تابع فعال‌سازی غیرخطی دو بعدی استفاده کرد.
توجه: در نهایت، ما می‌خواهیم وزن‌ها و بایاس‌های داخلی را مشابه یک شبکه عصبی متعارف آموزش دهیم.
معادله NODE را می‌توان به صورت زیر بیان کرد:

$\frac{dx(t)}{dt} = f(x(t),t;\theta)$

در اینجا،  $\theta$ نمایانگر پارامترهای داخلی یعنی وزن‌ها و بایاس‌ها است.

توضیح اضافی:
در معادلات دیفرانسیل عادی عصبی (NODE)، یک شبکه عصبی به عنوان تابع \( f \) عمل می‌کند که می‌تواند تغییرات در حالت سیستم دینامیکی را در طول زمان مدل کند. این شبکه با استفاده از وزن‌ها و بایاس‌های پارامتری، تغییرات پیوسته را یاد می‌گیرد و به ما امکان می‌دهد که سیستم‌های پیچیده‌تری را مدل کنیم. به عبارت دیگر، ما به جای استفاده از لایه‌های گسسته، از شبکه‌های عصبی استفاده می‌کنیم تا معادلات دیفرانسیل را حل کنیم و یک نمای پیوسته از تغییرات در زمان به دست آوریم.

## گذر به جلو (Forward-Pass)
در شبکه‌های عصبی مصنوعی معمولی، خروجی لایه فعلی ورودی لایه بعدی است. 
